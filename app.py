import streamlit as st
from PIL import Image, ImageOps
import torch
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModel
import numpy as np
import plotly.graph_objects as go

st.set_page_config(layout="centered", page_title="Fashion Spectrum", page_icon="ğŸ‘—")

# --- 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
# Streamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œä¸­ã«ä¸€åº¦ã ã‘ãƒªã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒUIã‚’æ“ä½œã™ã‚‹ãŸã³ã«å†èª­ã¿è¾¼ã¿ã•ã‚Œã‚‹ã®ã‚’é˜²ãã€é«˜é€ŸåŒ–ã—ã¾ã™ã€‚
@st.cache_resource
def load_resources():
    """
    ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã€ãŠã‚ˆã³ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Returns:
        tuple: å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ï¼ˆãƒ‡ãƒã‚¤ã‚¹ã€ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚¹ã‚¿ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰
    """
    print("âœ… ãƒªã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_name = "openai/clip-vit-base-patch32"

    # Hugging Faceã‹ã‚‰CLIPãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’èª­ã¿è¾¼ã¿
    processor = AutoProcessor.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)

    # ã‚¹ã‚¿ã‚¤ãƒ«ææ¡ˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å®šç¾©ã—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    # UIè¡¨ç¤ºã‚‚è‹±èªã«çµ±ä¸€ã™ã‚‹ãŸã‚ã€ã‚«ãƒ†ã‚´ãƒªåã¯è‹±èªã«å¤‰æ›´
    style_categories = {
        "Style": ["streetwear", "vintage", "modern", "sporty", "elegant", "preppy", "minimalist", "punk", "gothic", "hippie", "grunge"],
        "Color": ["red", "blue", "green", "yellow", "black", "white", "pink", "purple", "orange", "brown", "gray"]
    }

    fashion_styles = []
    for category in style_categories.keys():
        fashion_styles.extend(style_categories[category])

    text_inputs = processor(text=fashion_styles, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        style_features = model.get_text_features(**text_inputs)

    print("âœ… èª­ã¿è¾¼ã¿å®Œäº†ï¼")
    return device, processor, model, fashion_styles, style_categories, style_features


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒªã‚½ãƒ¼ã‚¹ã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚€
device, processor, model, fashion_styles, style_categories, style_features = load_resources()

# --- 2. ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def calculate_centroid_vector(uploaded_images, weights):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®é‡ã¿ä»˜ã‘å¹³å‡ï¼ˆé‡å¿ƒï¼‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    
    Args:
        uploaded_images (list): ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPILç”»åƒã®ãƒªã‚¹ãƒˆ
        weights (list): å„ç”»åƒã®é‡è¦åº¦ã‚’ç¤ºã™é‡ã¿ã®ãƒªã‚¹ãƒˆ
        
    Returns:
        torch.Tensor: è¨ˆç®—ã•ã‚ŒãŸé‡å¿ƒãƒ™ã‚¯ãƒˆãƒ«
    """
    # é‡ã¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)

    # é‡ã¿ã®åˆè¨ˆãŒ0ã®å ´åˆã¯è­¦å‘Šã‚’å‡ºã—ã¦çµ‚äº†
    if weights_tensor.sum() == 0:
        st.warning("All weights are 0. Please set at least one image weight greater than 0.")
        return None

    # å„ç”»åƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
    all_query_features = []
    for image in uploaded_images:
        inputs = processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
        all_query_features.append(image_features)

    # é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã®åˆè¨ˆã‚’è¨ˆç®—
    weighted_features = [feat * weight for feat, weight in zip(all_query_features, weights_tensor)]
    weighted_sum = torch.sum(torch.stack(weighted_features), dim=0)

    # é‡å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ã—ã€æ­£è¦åŒ–
    query_features_centroid = weighted_sum / weights_tensor.sum()
    query_features_centroid /= query_features_centroid.norm(dim=-1, keepdim=True)
    return query_features_centroid


def display_style_analysis(query_features_centroid):
    """
    é‡å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã€ã‚¹ã‚¿ã‚¤ãƒ«ã®ç³»çµ±ã‚„ã‚«ãƒ©ãƒ¼ã®åˆ†æçµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        query_features_centroid (torch.Tensor): è¨ˆç®—ã•ã‚ŒãŸé‡å¿ƒãƒ™ã‚¯ãƒˆãƒ«
    """
    st.header("Analysis Results (Style, Color, etc.)")
    st.write("We analyzed the attributes that describe your uploaded outfits.")

    for category_name, attributes in style_categories.items():
        st.subheader(category_name)
        for attribute in attributes:
            try:
                attribute_index = fashion_styles.index(attribute)
                similarity_score = F.cosine_similarity(
                    query_features_centroid,
                    style_features[attribute_index].unsqueeze(0)
                ).item()
                st.write(f"**{attribute}**")
                st.progress(similarity_score)
            except ValueError:
                continue


# --- 3. Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ¬ä½“ ---
def main():
    """
    Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚UIã®æ§‹ç¯‰ã¨å‡¦ç†ã®æµã‚Œã‚’å®šç¾©ã—ã¾ã™ã€‚
    """
    st.title("Fashion Style Spectrum")
    st.write("Decompose outfit images into attributes such as style, color, and silhouette.")

    uploaded_files = st.file_uploader(
        "Upload image(s)...",
        type=["jpg", "jpeg", "png"],
        accept_multiple_files=True
    )

    if uploaded_files:
        st.markdown("---")
        st.subheader("Uploaded Images & Weighting")

        query_images = []
        weights = []

        n_cols = min(4, len(uploaded_files))
        cols = st.columns(n_cols)

        preview_size = (1000, 1500)  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒã®ã‚µã‚¤ã‚ºã‚’æŒ‡å®š

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒã‚’è¡¨ç¤ºã—ã€é‡ã¿ä»˜ã‘ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’é…ç½®
        # åˆ—ã®æ•°ã‚’å‹•çš„ã«èª¿æ•´ã€ç”»åƒã‚’ä¸­å¤®ã‚¯ãƒ­ãƒƒãƒ—ã—ã¦è¡¨ç¤º
        for i, uploaded_file in enumerate(uploaded_files):
            image = Image.open(uploaded_file).convert("RGB")
                # è¡¨ç¤ºç”¨ã ã‘ä¸­å¤®ã‚¯ãƒ­ãƒƒãƒ—ã—ã¦ã‚µã‚¤ã‚ºçµ±ä¸€ï¼ˆå…ƒç”»åƒimageã¯å¤‰æ›´ã—ãªã„ï¼‰
            preview = ImageOps.fit(
                image,
                preview_size,
                method=Image.Resampling.LANCZOS,
                centering=(0.5, 0.5),
            )
            with cols[i % n_cols]:
                st.image(preview)
                weight = st.slider(
                    label="",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.5,
                    step=0.05,
                    key=f"slider_{uploaded_file.name}",
                    label_visibility="collapsed"
                )
            query_images.append(image)
            weights.append(weight)

        # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
        st.markdown("---")
        if st.button("Run analysis"):
            with st.spinner("Analyzing..."):
                # é‡å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
                query_features_centroid = calculate_centroid_vector(query_images, weights)

                # è¨ˆç®—ãŒæˆåŠŸã—ãŸå ´åˆã«ã®ã¿æ¬¡ã®å‡¦ç†ã‚’å®Ÿè¡Œ
                if query_features_centroid is not None:
                    # å„åˆ†æçµæœã‚’è¡¨ç¤º
                    st.markdown("---")
                    display_style_analysis(query_features_centroid)


# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹å§‹ç‚¹
if __name__ == "__main__":
    main()
